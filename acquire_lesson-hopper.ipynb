{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition with Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soup Methods\n",
    "\n",
    "- `soup.select('.class')`: to get all the elements with class `class`\n",
    "- `soup.select_one('.class')`: to get the first element with class `class`\n",
    "- `soup.h2`: to get the first `h2` element\n",
    "- `soup.find_all('h2')`: to get all the elements with tag name of `h2`\n",
    "- `soup('h2')` : same as `find_all` method above\n",
    "- `soup.find('h2')`: finds the first matching element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make the request. The response is a bunch of html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://web-scraping-demo.zgulde.net/news')\n",
    "html = response.text\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make more sense of that html with the beautiful soup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can switch between the browser and python and try out different ways of getting different parts of the html document.\n",
    "\n",
    "We can leverage Google Chrome's developer tools by right clicking and choosing \"Inspect\". We can then use this html document inspector to help us with our web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use beautifulsoup methods to extract necessary content from an article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together: Make a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_news(article):\n",
    "#     headline = \n",
    "#     date = \n",
    "#     author = \n",
    "#     content = \n",
    "#     content = \n",
    "    \n",
    "#     return {\n",
    "#         'headline': headline, 'date': date, 'author': author,\n",
    "#         'content': content\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://web-scraping-demo.zgulde.net/people', headers={'user-agent': 'Codeup DS Hoppper'})\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_person(person):\n",
    "#     name = \n",
    "#     quote = \n",
    "#     email = \n",
    "#     phone = \n",
    "#     address = \n",
    "\n",
    "    \n",
    "#     return {\n",
    "#         'name': name, 'quote': quote, 'email': email,\n",
    "#         'phone': phone,\n",
    "#         'address': address\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the persons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Etiquitte\n",
    "\n",
    "- respect the `robots.txt` file if present\n",
    "\n",
    "    * [Wikipedia: Robots exclusion standard](https://en.wikipedia.org/wiki/Robots_exclusion_standard)\n",
    "    * [robotstxt.org](http://www.robotstxt.org/robotstxt.html)\n",
    "    * [codeup's robots.txt](https://codeup.com/robots.txt)\n",
    "\n",
    "- use a descriptive user agent\n",
    "\n",
    "    ```python\n",
    "    requests.get('http://example.com', headers={'user-agent': 'codeup data science germain cohort'})\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "#### Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog(http://codeup.com/blog/) and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://codeup.com/blog/', headers={'user-agent': 'Codeup DS Hopper'})\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
